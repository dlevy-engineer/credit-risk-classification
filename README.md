# credit-risk-classification
A supervised learning project that uses historical lending activity from a peer-to-peer lending services company to build a model that can identify the creditworthiness of borrowers.

## Overview of the Analysis
This analysis is intended to compare the results of v1 and v2 logistic regression models fit to imbalanced loan data generated by a financial firm looking to implement ML tools into their credit risk screening process. The dataset provided contained information pertaining to both the loan (i.e. requested amount and the interest rate deemed appropriate by the firm) and the borrower's current financial wellbeing (i.e. income, current total debt, debt-to-income ratio, number of credit accounts and derogatory marks on credit history). The firm is interested in using the available data to classify loan applicants as creditworthy (`0`) or high-risk (`1`). 

The dataset provided was split into training and testing data in accordance with standard data governance practices, and the training set was used to generate two logistic regression models:
1. A standard logistic regression classifier trained on the scaled feature matrix.
    - The training target array contained $75,036$ creditworthy `0`s and $2,500$ high-risk `1`s, resulting in a highly imbalanced training set.
2. A logistic regression classifier trained on a bootstrapped training sample generated using the `RandomOverSampler` module from `imblearn` to account for an imbalance in creditworthy vs. high-risk loans in the dataset.
    - The training target array contained $56,271$ creditworthy `0`s and $56,271$ high-risk `1`s.

## Results

* **Machine Learning Model 1:**
The confusion matrix for v1 of our logistic regression model was as follows:
    ![Model 1 Confusion Matrixt](/images/model_v1_con_mat.png)

The model classification report for v1 of our logistic regression model was as follows:
    ![Model 1 Classification Report](/images/model_v1_report.png)

The fundamental question is: _How well does the logistic regression model predict both the `0` (healthy loan) and `1` (high-risk loan) labels?_

It seems that our model has high accuracy overall, which might initially lead us to believe that it is quite adept at its credit classification task.

Digging into classwise precision and recall, however, we see a different picture altogether. While high precision and recall scores on the healthy loan class indicate that the model accurately predicts healthy loans, we find that its performance on risky loans is lower, at 0.84 precision and 0.98 recall.

What does this mean more precisely? Well, let's consider the respective definitions of those terms:
- $\text{Recall} = \frac{\text{True Positive}}{\text{True Positive } + \text{ False Negative}}$

- $\text{Precision} = \frac{\text{True Positive}}{\text{True Positive } + \text{ False Positive}}$

In other words, 84% of the flags thrown by our system to indicate a high-risk loan were justified, while 16% were not. Furthermore, 98% of the loans that should have been flagged were caught by the model, while 2% were not.



* **Machine Learning Model 2:**
The confusion matrix for v1 of our logistic regression model was as follows:
    ![Model 2 Confusion Matrixt](/images/model_v2_con_mat.png)

The model classification report for v1 of our logistic regression model was as follows:
    ![Model 2 Classification Report](/images/model_v2_report.png)

Interestingly, the overall performance of our model seems to have improved slightly, while precision with respect to high-risk loans has decreased. In general, other performance indicators were not meningfully affected across the board.

## Summary
In determining which of our models are qualified for deployment in the firm's credit applications system, we must consider the real-world implications on the firm's bottom line, the loan application system users, and the financial system at large.

Critically, a 2% failure rate in v1 of our model has translated, in this test case, to the issuance of 10 risky loans. Without further data regarding the potential institutional and systemic impact of borrower default on these 10 loans, it would be impossible to make an informed statement about the sufficiency of this model to serve its intended purspose.

In this instance, it may be justified to say that v2 of our model is a better performer because we have flagged more credit risk cases than we did with the model trained only on our original data set. If the firm is concerned with eliminating issuance of risky loans altogether, neither model is sufficient. If the firm is interested in minimizing issuance of risky loans and reducing the need for human oversight of credit application screenings to mitigate human resource costs, v2 of our model may be the best solution to implement. If, however, it is not fiscally irresponsible to accept the error rate produced by our previous model, the firm may stand to generate more income with v1 of our model because it does not screen out as many qualified applicants (the firm failed to issue 12 additional loans to qualified applicants on the basis of model error in v2 compared with v1).

As is often the case in finance, the question for management to consider is: _How much risk is too much?_