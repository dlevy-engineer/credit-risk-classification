# credit-risk-classification
A supervised learning project that uses historical lending activity from a peer-to-peer lending services company to build a model that can identify the creditworthiness of borrowers.

## Overview of the Analysis
This analysis is intended to compare the results of V1 and V2 logistic regression models fit to imbalanced loan data generated by a financial firm looking to implement ML tools into their credit risk screening process. The dataset provided contained information pertaining to both the loan (i.e. requested amount and the interest rate deemed appropriate by the firm) and the borrower's current financial wellbeing (i.e. income, current total debt, debt-to-income ratio, number of credit accounts and derogatory marks on credit history). The firm is interested in using the available data to classify loan applicants as creditworthy (`0`) or high-risk (`1`). 

The dataset provided was split into training and testing data in accordance with standard data governance practices, and the training set was used to generate two logistic regression models:
1. A standard logistic regression classifier trained on the scaled feature matrix.
    - The training target array contained $75,036$ creditworthy `0`s and $2,500$ high-risk `1`s, resulting in a highly imbalanced training set.
2. A logistic regression classifier trained on a bootstrapped training sample generated using the `RandomOverSampler` module from `imblearn` to account for an imbalance in creditworthy vs. high-risk loans in the dataset.
    - The training target array contained $56,271$ creditworthy `0`s and $56,271$ high-risk `1`s.

## Results

* Machine Learning Model 1:
  * Description of Model 1 Accuracy, Precision, and Recall scores.

  The fundamental question is: _How well does the logistic regression model predict both the `0` (healthy loan) and `1` (high-risk loan) labels?_

It seems that our model has high accuracy overall, which might initially lead us to believe that it is quite adept at its credit classification task.

Digging into classwise precision and recall, however, we see a different picture altogether. While high precision and recall scores on the healthy loan class indicate that the model accurately predicts healthy loans, we find that its performance on risky loans is lower, at 0.84 precision and 0.98 recall.

What does this mean more precisely? Well, let's dig into the respective definitions of those terms:
- $\text{Recall} = \frac{\text{True Positive}}{\text{True Positive } + \text{ False Negative}}$

- $\text{Precision} = \frac{\text{True Positive}}{\text{True Positive } + \text{ False Positive}}$

In other words, 84% of the flags thrown by our system to indicate a high-risk loan were justified, while 16% were not. Furthermore, 98% of the loans that should have been flagged were caught by the model. Critically, we must realize that a 2% failure rate has translated, in this test case, to the issuance of 10 risky loans. Without further data regarding the potential institutional and systemic impact of borrower default on these 10 loans, it would be impossible to make an informed statement about the sufficiency of this model to serve its intended purspose.



* Machine Learning Model 2:
  * Description of Model 2 Accuracy, Precision, and Recall scores.

  Interestingly, the overall performance of our model seems to have improved slightly, while precision with respect to high-risk loans has decreased. I think that in this instance, it may be justified to say that our model is a better performer in this instance because we have flagged more credit risk cases than we did with the model trained only on our original data set. If, however, it is not fiscally irresponsible to accept the error rate produced by our previous model, the firm may stand to generate more income with the previous iteration of our model.

## Summary

